{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For eating, the stem is eat\n",
      "For eats, the stem is eat\n",
      "For eat, the stem is eat\n",
      "For ate, the stem is ate\n",
      "For adjustable, the stem is adjust\n",
      "For rafting, the stem is raft\n",
      "For ability, the stem is abil\n",
      "For meeting, the stem is meet\n"
     ]
    }
   ],
   "source": [
    "words = [\"eating\", \"eats\", \"eat\", \"ate\", \"adjustable\", \"rafting\", \"ability\", \"meeting\"]\n",
    "for word in words:\n",
    "    print(\"For {}, the stem is {}\".format(word, stemmer.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stemming not in spcay\n",
    "## Now for lemmatization use spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mando  ->  Mando\n",
      "talked  ->  talk\n",
      "for  ->  for\n",
      "3  ->  3\n",
      "hours  ->  hour\n",
      "although  ->  although\n",
      "talking  ->  talk\n",
      "is  ->  be\n",
      "n't  ->  not\n",
      "his  ->  his\n",
      "thing  ->  thing\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Mando talked for 3 hours although talking isn't his thing\")\n",
    "for word in doc:\n",
    "    print(word, \" -> \", word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bro -> Brother\n",
      ", -> ,\n",
      "you -> you\n",
      "wanna -> wanna\n",
      "go -> go\n",
      "? -> ?\n",
      "Brah -> Brother\n",
      ", -> ,\n",
      "do -> do\n",
      "n't -> not\n",
      "say -> say\n",
      "no -> no\n",
      "! -> !\n",
      "I -> I\n",
      "am -> be\n",
      "getting -> get\n",
      "exhausted -> exhaust\n"
     ]
    }
   ],
   "source": [
    "## Making a custom rule for lemmatisation\n",
    "ar = nlp.get_pipe('attribute_ruler')\n",
    "ar.add([[{\"TEXT\":\"Bro\"}], [{\"TEXT\": \"Brah\"}]], {\"LEMMA\" : \"Brother\"})\n",
    "\n",
    "doc = nlp(\"Bro, you wanna go? Brah, don't say no! I am getting exhausted\")\n",
    "for token in doc:\n",
    "    print(token.text, \"->\", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
